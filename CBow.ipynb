{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous Bag of Words (CBOW) Model Implementation:\n",
        "\n",
        "\n",
        ".   This example demonstrates a Python implementation of the Continuous Bag of Words (CBOW) model, a key component of the Word2Vec framework used for generating word embeddings. The CBOW model predicts a target word based on its surrounding context words, capturing semantic relationships in text. This  "
      ],
      "metadata": {
        "id": "67EtBKh5aZ_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for the CBOW model implementation\n",
        "import unicodedata\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "s0IKKwsjr_bX"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "u5w6GKjKj6X6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Setting Hyperparameters for the CBOW Model:"
      ],
      "metadata": {
        "id": "nzz6u8Zd6N4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer=90\n",
        "context_size=2"
      ],
      "metadata": {
        "id": "K3E-EbEkuRTb"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation:"
      ],
      "metadata": {
        "id": "kAF85vHae-SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences =\" The ancient castle stood tall against the backdrop of mist-covered mountains, its weathered stone walls bearing witness to centuries of history. Inside, torches flickered dimly, casting dancing shadows along the corridors lined with tapestries depicting epic battles and noble quests. Scholars huddled in corners, poring over ancient manuscripts filled with arcane knowledge. The aroma of hearty stew drifted from the castle kitchen, where cooks bustled about preparing meals fit for kings and knights. Outside, in the castle courtyard, knights practiced their swordsmanship under the watchful eye of seasoned masters. It was a place where legends were born and whispered secrets echoed through the halls.\""
      ],
      "metadata": {
        "id": "HMSniUMMTmgB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicodeToAscii(s):\n",
        "    \"\"\"\n",
        "    Convert Unicode string to plain ASCII by removing diacritics.\n",
        "    \"\"\"\n",
        "    return \"\".join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalizeString(s):\n",
        "    \"\"\"\n",
        "    Normalize string: lowercase, remove non-letters except punctuation, add space around punctuation.\n",
        "    \"\"\"\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "# Normalize the 'sentences' string\n",
        "sentences = normalizeString(sentences)\n"
      ],
      "metadata": {
        "id": "VtXTMRmzsDmu"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sorted list of unique words from the sentences\n",
        "vocab = sorted(list(set(sentences.split())))\n",
        "\n",
        "# Get the length of the vocabulary\n",
        "vocab_len = len(vocab)\n",
        "\n",
        "# Create dictionaries to map indices to words and words to indices\n",
        "index2word = {i: j for i, j in enumerate(vocab)}\n",
        "word2index = {j: i for i, j in enumerate(vocab)}\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "5KMUnZKzpCrk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building CBOW Data:\n",
        "The CBOW model requires context-target pairs for training. We define functions to generate context windows and corresponding target words from the tokenized sequences."
      ],
      "metadata": {
        "id": "CFj9wr8igz8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data and target lists\n",
        "data = []\n",
        "target = []\n",
        "\n",
        "# Loop through the sentence to create context and center words\n",
        "for i in range(2, len(sentences.split()) - 2):\n",
        "    context = [word2index[sentences.split()[i-2]], word2index[sentences.split()[i-1]], word2index[sentences.split()[i+1]], word2index[sentences.split()[i+2]]]\n",
        "    center = [word2index[sentences.split()[i]]]\n",
        "\n",
        "    # Convert context and center words to one-hot encoded tensors\n",
        "    a = torch.nn.functional.one_hot(torch.tensor(context), num_classes=vocab_len)\n",
        "    b = torch.nn.functional.one_hot(torch.tensor(center), num_classes=vocab_len)\n",
        "\n",
        "    # Append center word to target list and context to data list\n",
        "    target.append(center)\n",
        "    data.append(a)\n",
        "\n",
        "# Convert target and data to tensors\n",
        "target = torch.tensor(target).squeeze()\n",
        "data = np.array(data)\n",
        "input = torch.from_numpy(data)\n",
        "\n",
        "input, target"
      ],
      "metadata": {
        "id": "1gO0Oj29-iae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture:\n",
        "The CBOW model architecture consists of an embedding layer followed by a linear layer. We define it by inheriting the nn.Module Pytorch Class as WordEmbeddings. The output of the this model is going to be a word embedding of side of vocabulary."
      ],
      "metadata": {
        "id": "55ceaQ6mhSng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cbow(nn.Module):\n",
        "    def __init__(self, input_size, n):\n",
        "        \"\"\"\n",
        "        Initialize the CBOW model with input and hidden layer sizes.\n",
        "\n",
        "        Args:\n",
        "        input_size (int): The size of the input layer.\n",
        "        n (int): The size of the hidden layer (embedding dimension).\n",
        "        \"\"\"\n",
        "        super(cbow, self).__init__()\n",
        "        self.n = n\n",
        "        self.linear1 = nn.Linear(input_size, n)  # Embedding layer\n",
        "        self.linear2 = nn.Linear(n, input_size)  # Prediction layer\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Forward pass of the CBOW model.\n",
        "\n",
        "        Args:\n",
        "        input (Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "        Tensor: The predicted output.\n",
        "        \"\"\"\n",
        "        input = input.float()\n",
        "        layer_1 = self.linear1(input)  # Project to embedding space\n",
        "        ave = torch.sum(layer_1, dim=1) / (2 * context_size)  # Average embeddings\n",
        "        layer_2 = self.linear2(ave)  # Map to vocabulary space\n",
        "        y_pred = torch.softmax(layer_2, dim=1)  # Softmax for prediction\n",
        "        return y_pred\n",
        "\n",
        "# Note: The weights of the linear1 layer will be used for word embeddings.\n",
        "# After training, the weights of this layer represent the learned word vectors."
      ],
      "metadata": {
        "id": "ffxDfil2kk4v"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model:"
      ],
      "metadata": {
        "id": "R6IWfkDGjQ1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the CBOW model\n",
        "model = cbow(vocab_len, hidden_layer)\n",
        "\n",
        "# Define the optimizer and loss criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
        "criterion = nn.NLLLoss()  # Assuming NLLLoss is the appropriate criterion for this task\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Number of epochs for training\n",
        "epochs = 10000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y = model(input)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(y, target)\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: Compute gradient of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss value for the current epoch\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "rMxZbEv7EOqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Analysis:"
      ],
      "metadata": {
        "id": "J2hk9Wico4Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert model output probabilities to predicted class indices\n",
        "_, predicted_classes = torch.max(y, 1)  # Get the indices of the max values along the class dimension\n",
        "\n",
        "# Calculate the number of correct predictions\n",
        "correct_predictions = (predicted_classes == target).sum().item()\n",
        "\n",
        "# Calculate total number of samples\n",
        "total_samples = len(target)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = (correct_predictions / total_samples) * 100\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om5HMCKXn_VW",
        "outputId": "0617b0f4-fed4-43ab-be52-2afdaa880d5b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract the learned word embeddings from the CBOW model.\n",
        "# `model.linear1.weight.data` contains the weights of the linear layer `linear1`,\n",
        "# which represent the word embeddings. These embeddings are learned during training\n",
        "# and are used to capture the semantic meaning of words in the vector space.\n",
        "word_embeddings = model.linear1.weight.data\n"
      ],
      "metadata": {
        "id": "nrpDvWqnRikD"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Word Embeddings in Vector Space:\n",
        "This section visualizes word embeddings obtained from the CBOW model in a reduced 2D vector space. By applying dimensionality reduction techniques like PCA, we can project high-dimensional word vectors into two dimensions. The resulting scatter plot illustrates how words are distributed and clustered based on their semantic similarities. Interactive tools like Plotly enhance the exploration process, allowing for zooming and panning to examine relationships between words and better understand the underlying structure of the embedding space."
      ],
      "metadata": {
        "id": "SD5Aon8sw2Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in the index2word dictionary with \"<NaW>\" (Not a Word) tokens to match the length of index2word with the hidden layer size\n",
        "for i in range(vocab_len, hidden_layer):\n",
        "    index2word[i] = \"<NaW>\"\n",
        ""
      ],
      "metadata": {
        "id": "2yw1v3jbzxkG"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare the data for Plotly\n",
        "df = pd.DataFrame(embeddings_2d, columns=['Component 1', 'Component 2'])\n",
        "df['Word'] = [index2word[i] for i in range(hidden_layer)]\n",
        "\n",
        "# Create the scatter plot\n",
        "fig = px.scatter(df, x='Component 1', y='Component 2', text='Word', title='Word Embeddings Visualization (PCA)')\n",
        "\n",
        "# Update layout for better readability\n",
        "fig.update_traces(textposition='top center')\n",
        "fig.update_layout(\n",
        "    xaxis_title='Component 1',\n",
        "    yaxis_title='Component 2',\n",
        "    title='Word Embeddings Visualization (PCA)',\n",
        "    template='plotly_white'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "2bU2wk2Xzi9j",
        "outputId": "a4c10a59-dcf0-417e-b029-3cbc1840a8aa"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"e73ef3a8-d6ff-4d00-92b4-1236fe4b8718\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e73ef3a8-d6ff-4d00-92b4-1236fe4b8718\")) {                    Plotly.newPlot(                        \"e73ef3a8-d6ff-4d00-92b4-1236fe4b8718\",                        [{\"hovertemplate\":\"Component 1=%{x}\\u003cbr\\u003eComponent 2=%{y}\\u003cbr\\u003eWord=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[\"a\",\"about\",\"against\",\"along\",\"ancient\",\"and\",\"arcane\",\"aroma\",\"backdrop\",\"battles\",\"bearing\",\"born\",\"bustled\",\"casting\",\"castle\",\"centuries\",\"cooks\",\"corners\",\"corridors\",\"courtyard\",\"covered\",\"dancing\",\"depicting\",\"dimly\",\"drifted\",\"echoed\",\"epic\",\"eye\",\"filled\",\"fit\",\"flickered\",\"for\",\"from\",\"halls\",\"hearty\",\"history\",\"huddled\",\"in\",\"inside\",\"it\",\"its\",\"kings\",\"kitchen\",\"knights\",\"knowledge\",\"legends\",\"lined\",\"manuscripts\",\"masters\",\"meals\",\"mist\",\"mountains\",\"noble\",\"of\",\"outside\",\"over\",\"place\",\"poring\",\"practiced\",\"preparing\",\"quests\",\"scholars\",\"seasoned\",\"secrets\",\"shadows\",\"stew\",\"stone\",\"stood\",\"swordsmanship\",\"tall\",\"tapestries\",\"the\",\"their\",\"through\",\"to\",\"torches\",\"under\",\"walls\",\"was\",\"watchful\",\"weathered\",\"were\",\"where\",\"whispered\",\"with\",\"witness\",\"\\u003cNaW\\u003e\",\"\\u003cNaW\\u003e\",\"\\u003cNaW\\u003e\",\"\\u003cNaW\\u003e\"],\"x\":[5.346292018890381,3.2151527404785156,-5.185105800628662,1.7154815196990967,-4.895453929901123,-3.7038938999176025,1.0314830541610718,-0.917748749256134,1.9357473850250244,-5.092019081115723,6.776629447937012,-4.500858306884766,-4.954188823699951,-3.4673006534576416,1.8805383443832397,0.3569243550300598,2.9695985317230225,-3.623359203338623,-5.89840030670166,3.3516955375671387,3.1122682094573975,-3.195361375808716,1.3472270965576172,-2.3451011180877686,4.477747440338135,4.651027679443359,1.6418694257736206,-1.4587507247924805,3.5979485511779785,1.9589638710021973,-4.484915256500244,-1.110474944114685,4.235352516174316,-4.274123191833496,0.3406379222869873,-0.4734250605106354,-0.9059551954269409,-0.8668577671051025,0.4580432176589966,-1.0539928674697876,4.771197319030762,-3.1238763332366943,-4.486484050750732,-4.647469520568848,-1.7569921016693115,-2.100168228149414,-2.6310524940490723,-1.3416589498519897,3.2779362201690674,-2.2516884803771973,5.160449504852295,1.7406046390533447,0.8975270986557007,-1.1623427867889404,0.5905629396438599,0.20750829577445984,2.666231632232666,3.8107173442840576,-2.017749547958374,3.9054343700408936,-3.57711124420166,5.087883949279785,-1.1438994407653809,-0.9895502328872681,-3.434253454208374,0.19647550582885742,1.377522349357605,-0.6640399694442749,5.312469959259033,4.084731101989746,-4.171877384185791,-3.1262733936309814,-0.4379050135612488,-5.835774898529053,4.390129566192627,-2.315338134765625,5.084432125091553,-1.8974980115890503,-3.332326650619507,-1.3830958604812622,4.968759059906006,1.2208459377288818,-1.4967353343963623,1.8331196308135986,1.8105041980743408,-6.5630364418029785,-0.08478227257728577,0.7993736267089844,6.183225154876709,4.602000713348389],\"xaxis\":\"x\",\"y\":[-1.043894648551941,4.739508152008057,1.6687077283859253,-4.406627655029297,-0.42422348260879517,-0.28751444816589355,-4.820624828338623,-4.0392746925354,-3.4118335247039795,0.3619301915168762,1.93681001663208,-0.259956032037735,1.890926718711853,-3.2860960960388184,-3.4912631511688232,2.9543774127960205,6.250691890716553,-2.715758800506592,0.7035450339317322,1.7962844371795654,5.641221046447754,1.6386455297470093,-1.9469729661941528,-3.6089303493499756,-0.8444432020187378,2.5256450176239014,-1.8254294395446777,5.4409098625183105,-1.2434786558151245,0.7924180626869202,2.101048231124878,4.40337610244751,-0.5266633033752441,-1.1398950815200806,-4.638238906860352,-4.270042419433594,2.478390693664551,-2.7855677604675293,-0.6949654817581177,2.725482940673828,0.4980083107948303,0.5186412930488586,0.6894382238388062,-1.6826300621032715,1.648234486579895,0.35510218143463135,-4.432727813720703,-3.6670432090759277,-4.4173502922058105,-3.5959420204162598,-1.6103129386901855,-2.9379992485046387,4.315301895141602,-4.056944370269775,0.26816681027412415,6.370290279388428,-1.3332823514938354,-1.7999253273010254,-2.079559803009033,-1.4596948623657227,0.9331976175308228,3.0734832286834717,-2.169590711593628,-3.1547799110412598,0.9166377782821655,1.285908818244934,4.417774677276611,3.7055156230926514,1.3385827541351318,-0.04671217501163483,2.040792465209961,4.929856300354004,4.051781177520752,-1.8348087072372437,-1.2477874755859375,2.891080379486084,-3.040104866027832,-1.8764907121658325,-0.46414387226104736,-1.87458074092865,1.2018243074417114,-2.4236037731170654,0.6268361806869507,3.996722459793091,-4.2101850509643555,2.7304670810699463,0.17750731110572815,3.165153980255127,-2.5526938438415527,3.4843640327453613],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Component 1\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Component 2\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Word Embeddings Visualization (PCA)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e73ef3a8-d6ff-4d00-92b4-1236fe4b8718');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "In this example, we implemented the Continuous Bag of Words (CBOW) model using PyTorch. The model takes context words as input and predicts the center word, which is useful for creating word embeddings."
      ],
      "metadata": {
        "id": "VzkX89MO7RPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary:\n",
        "1). Preprocessing: We normalized the input sentences and created mappings between words and indices.                                 \n",
        "2). Model Implementation: We built a CBOW model using linear layers in PyTorch.              \n",
        "3). Training: We trained the model using a specified number of epochs and calculated the loss at each step.                                                           \n",
        "4). Visualization: We visualized the word embeddings using PCA and Plotly for better interpretability."
      ],
      "metadata": {
        "id": "W4jg7YOe7aun"
      }
    }
  ]
}